{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "In this notebook we will be implementing batch gradient descent to train for Logistic Regression.\n",
    "\n",
    "Logistic Regression is a predictive algorithm in Machine Learning that is used for binary classification. It predicts the probability of a class and then classifies it based on the predictor variables' values. Logistic regression uses logistic sigmoid activation, in contrast to linear regression, which\n",
    "uses the identity function. As we've seen before, the output of the logistic sigmoid is in the\n",
    "(0,1) range and can be interpreted as a probability function. We can use logistic regression\n",
    "for a 2-class (binary) classification problem, where our target, t, can have two values,\n",
    "usually 0 and 1 for the two corresponding classes. These discrete values shouldn't be\n",
    "confused with the values of the logistic sigmoid function, which is a continuous real-valued\n",
    "function between 0 and 1. The value of the sigmoid function represents the probability that\n",
    "the output is in class 0 or class 1.\n",
    "\n",
    "A logistic function or logistic curve is a common S-shaped curve (sigmoid curve) with the equation:\n",
    "\n",
    "$${\\displaystyle f(x)={\\frac {L}{1+e^{-k(x-x_{0})}}},}$$\n",
    "\n",
    "In statistics, logistic regression is a predictive analysis that used to describe data and to explain the relationship between one dependent binary variable and one or more nominal, ordinal, interval or ratio-level independent variables. Here is the basic formula of logistic regression:\n",
    "\n",
    "$$ln({\\frac{P}{1-P})}=a+bX$$\n",
    "\n",
    "$${\\frac{P}{1-P}}=e^{a+bX}$$\n",
    "\n",
    "$$P={\\frac{e^{a+bX}}{1+e^{a+bX}}}$$\n",
    "\n",
    "We will be implementing these formulas to determine the likelyhood of college applicants being admitted to a univeristy. For this we will need the following packages:\n",
    "* CSV [documentation](https://juliadata.github.io/CSV.jl/stable/)\n",
    "* DataFrames [documentation](https://juliadata.github.io/DataFrames.jl/stable/)\n",
    "_______"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>gmat</th><th>gpa</th><th>work_experience</th><th>admitted</th></tr><tr><th></th><th>Int64</th><th>Float64</th><th>Int64</th><th>Int64</th></tr></thead><tbody><p>10 rows × 4 columns</p><tr><th>1</th><td>780</td><td>4.0</td><td>3</td><td>1</td></tr><tr><th>2</th><td>750</td><td>3.9</td><td>4</td><td>1</td></tr><tr><th>3</th><td>690</td><td>3.3</td><td>3</td><td>0</td></tr><tr><th>4</th><td>710</td><td>3.7</td><td>5</td><td>1</td></tr><tr><th>5</th><td>680</td><td>3.9</td><td>4</td><td>0</td></tr><tr><th>6</th><td>730</td><td>3.7</td><td>6</td><td>1</td></tr><tr><th>7</th><td>690</td><td>2.3</td><td>1</td><td>0</td></tr><tr><th>8</th><td>720</td><td>3.3</td><td>4</td><td>1</td></tr><tr><th>9</th><td>740</td><td>3.3</td><td>5</td><td>1</td></tr><tr><th>10</th><td>690</td><td>1.7</td><td>1</td><td>0</td></tr></tbody></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|cccc}\n",
       "\t& gmat & gpa & work\\_experience & admitted\\\\\n",
       "\t\\hline\n",
       "\t& Int64 & Float64 & Int64 & Int64\\\\\n",
       "\t\\hline\n",
       "\t1 & 780 & 4.0 & 3 & 1 \\\\\n",
       "\t2 & 750 & 3.9 & 4 & 1 \\\\\n",
       "\t3 & 690 & 3.3 & 3 & 0 \\\\\n",
       "\t4 & 710 & 3.7 & 5 & 1 \\\\\n",
       "\t5 & 680 & 3.9 & 4 & 0 \\\\\n",
       "\t6 & 730 & 3.7 & 6 & 1 \\\\\n",
       "\t7 & 690 & 2.3 & 1 & 0 \\\\\n",
       "\t8 & 720 & 3.3 & 4 & 1 \\\\\n",
       "\t9 & 740 & 3.3 & 5 & 1 \\\\\n",
       "\t10 & 690 & 1.7 & 1 & 0 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m10×4 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m gmat  \u001b[0m\u001b[1m gpa     \u001b[0m\u001b[1m work_experience \u001b[0m\u001b[1m admitted \u001b[0m\n",
       "\u001b[1m     \u001b[0m│\u001b[90m Int64 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Int64           \u001b[0m\u001b[90m Int64    \u001b[0m\n",
       "─────┼───────────────────────────────────────────\n",
       "   1 │   780      4.0                3         1\n",
       "   2 │   750      3.9                4         1\n",
       "   3 │   690      3.3                3         0\n",
       "   4 │   710      3.7                5         1\n",
       "   5 │   680      3.9                4         0\n",
       "   6 │   730      3.7                6         1\n",
       "   7 │   690      2.3                1         0\n",
       "   8 │   720      3.3                4         1\n",
       "   9 │   740      3.3                5         1\n",
       "  10 │   690      1.7                1         0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using CSV\n",
    "using DataFrames\n",
    "\n",
    "data = CSV.read(\"candidates_data.csv\", DataFrame)\n",
    "first(data, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting Features\n",
    "Here, we need to divide the given columns into two types of variables dependent(or target variable) and independent variable(or feature variables)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = [[x[1], x[2]] for x in zip(data.gmat, data.gpa)]\n",
    "y_data = [x for x in data.admitted];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code below we will model the probability using the logistic regression function: $$g(z)=1/(1+e^{-x})$$ \n",
    "\n",
    "We will us cross-entrophy  to define a loss function $$(-ylog\\hat{y}-(1-y)log(1-\\hat{y}))$$ \n",
    "\n",
    "along with the average loss. $$-1/N \\sum_{n=1}^{N} [y_n log\\hat{y}_n + (1 - y_n) log (1 - \\hat{y}_n)]$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "average_loss (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "σ(x) = 1/(1+exp(-x))\n",
    "\n",
    "function cross_entrophy_loss(x, y, w, b)\n",
    "    return -y*log(σ(w'x + b)) -(1-y)*log(1 - σ(w'x+b))\n",
    "end\n",
    "\n",
    "function average_loss(features, labels, w, b)\n",
    "    N = length(features)\n",
    "    return (1/N)*sum([cross_entrophy_loss(features[i], labels[i], w, b) for i = 1:N])\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "batch_gradient_descent (generic function with 1 method)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function batch_gradient_descent(features, labels, w, b, α)\n",
    "    del_w = [0.0 for i = 1:length(w)]\n",
    "    del_b = 0.0\n",
    "    \n",
    "    N = length(features)\n",
    "    \n",
    "    for i = 1;N\n",
    "        del_w += (σ(w'features[i]+b) - labels[i])*features[i]\n",
    "        del_b += (σ(w'features[i]+b) - labels[i])\n",
    "    end\n",
    "    \n",
    "    w = w - α*del_w\n",
    "    b = b - α*del_b\n",
    "    \n",
    "    return w, b\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The initial cost is: 0.6931471805599451\n"
     ]
    }
   ],
   "source": [
    "w = [0.0, 0.0]\n",
    "b = 0.0\n",
    "println(\"The initial cost is: \", average_loss(x_data, y_data, w, b))\n",
    "\n",
    "         #notice it getting smaller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train_batch_gradient_descent (generic function with 1 method)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function train_batch_gradient_descent(features,labels, w,b,α, epochs)\n",
    "    for i = 1:epochs \n",
    "        \n",
    "        w, b = batch_gradient_descent(features, labels, w,b,α)\n",
    "        if i == 1\n",
    "            println(\"Epochs \", i , \" with loss \", average_loss(x_data, y_data,w,b))\n",
    "        end\n",
    "        if i == epochs/10\n",
    "            println(\"Epochs \", i , \" with loss \", average_loss(x_data, y_data,w,b))\n",
    "        end\n",
    "        if i == epochs/8\n",
    "            println(\"Epochs \", i , \" with loss \", average_loss(x_data, y_data,w,b))\n",
    "        end\n",
    "        if i == epochs/4\n",
    "            println(\"Epochs \", i , \" with loss \", average_loss(x_data, y_data,w,b))\n",
    "        end\n",
    "        if i == epochs/2\n",
    "            println(\"Epochs \", i , \" with loss \", average_loss(x_data, y_data,w,b))\n",
    "        end\n",
    "        if i == epochs\n",
    "            println(\"Epochs \", i , \" with loss \", average_loss(x_data, y_data,w,b))\n",
    "        end\n",
    "        end \n",
    "    return w,b\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs 1 with loss 0.6931121727730898\n",
      "Epochs 1000 with loss 1.7201365047552426\n",
      "Epochs 1250 with loss 1.8100004134107341\n",
      "Epochs 2500 with loss 2.0925733760893857\n",
      "Epochs 5000 with loss 2.3779706486277505\n",
      "Epochs 10000 with loss 2.6646298882428856\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.008207757919005253, 4.209106625130911e-5], 1.0522766562827277e-5)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = [0.0, 0.0]\n",
    "b = 0.0\n",
    "\n",
    "w, b = train_batch_gradient_descent(x_data, y_data, w, b, 0.0000001, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predict (generic function with 1 method)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function predict(x, y, w, b)\n",
    "    if σ(w'x+b) >= .5\n",
    "        println(\"Predict Accepted\")\n",
    "        y == 1 ? println(\"Was Accepted\") : println(\"Was Not Accepted\")\n",
    "    else\n",
    "        println(\"Predict Not Accepted\")\n",
    "        y == 1 ? println(\"Was Accepted\") : println(\"Was Not Accepted\")\n",
    "        \n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict Accepted\n",
      "Was Accepted\n",
      "\n",
      "Predict Accepted\n",
      "Was Accepted\n",
      "\n",
      "Predict Accepted\n",
      "Was Not Accepted\n",
      "\n",
      "Predict Accepted\n",
      "Was Accepted\n",
      "\n",
      "Predict Accepted\n",
      "Was Not Accepted\n",
      "\n",
      "Predict Accepted\n",
      "Was Accepted\n",
      "\n",
      "Predict Accepted\n",
      "Was Not Accepted\n",
      "\n",
      "Predict Accepted\n",
      "Was Accepted\n",
      "\n",
      "Predict Accepted\n",
      "Was Accepted\n",
      "\n",
      "Predict Accepted\n",
      "Was Not Accepted\n",
      "\n",
      "Predict Accepted\n",
      "Was Not Accepted\n",
      "\n",
      "Predict Accepted\n",
      "Was Accepted\n",
      "\n",
      "Predict Accepted\n",
      "Was Accepted\n",
      "\n",
      "Predict Accepted\n",
      "Was Not Accepted\n",
      "\n",
      "Predict Accepted\n",
      "Was Accepted\n",
      "\n",
      "Predict Accepted\n",
      "Was Not Accepted\n",
      "\n",
      "Predict Accepted\n",
      "Was Not Accepted\n",
      "\n",
      "Predict Accepted\n",
      "Was Accepted\n",
      "\n",
      "Predict Accepted\n",
      "Was Not Accepted\n",
      "\n",
      "Predict Accepted\n",
      "Was Not Accepted\n",
      "\n",
      "Predict Accepted\n",
      "Was Accepted\n",
      "\n",
      "Predict Accepted\n",
      "Was Not Accepted\n",
      "\n",
      "Predict Accepted\n",
      "Was Not Accepted\n",
      "\n",
      "Predict Accepted\n",
      "Was Not Accepted\n",
      "\n",
      "Predict Accepted\n",
      "Was Not Accepted\n",
      "\n",
      "Predict Accepted\n",
      "Was Accepted\n",
      "\n",
      "Predict Accepted\n",
      "Was Accepted\n",
      "\n",
      "Predict Accepted\n",
      "Was Not Accepted\n",
      "\n",
      "Predict Accepted\n",
      "Was Accepted\n",
      "\n",
      "Predict Accepted\n",
      "Was Accepted\n",
      "\n",
      "Predict Accepted\n",
      "Was Not Accepted\n",
      "\n",
      "Predict Accepted\n",
      "Was Not Accepted\n",
      "\n",
      "Predict Accepted\n",
      "Was Accepted\n",
      "\n",
      "Predict Accepted\n",
      "Was Accepted\n",
      "\n",
      "Predict Accepted\n",
      "Was Accepted\n",
      "\n",
      "Predict Accepted\n",
      "Was Not Accepted\n",
      "\n",
      "Predict Accepted\n",
      "Was Not Accepted\n",
      "\n",
      "Predict Accepted\n",
      "Was Not Accepted\n",
      "\n",
      "Predict Accepted\n",
      "Was Not Accepted\n",
      "\n",
      "Predict Accepted\n",
      "Was Accepted\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i = 1:length(x_data)\n",
    "    predict(x_data[i], y_data[i], w, b)\n",
    "    println()\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predict (generic function with 1 method)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function predict(x, y, w, b)\n",
    "    if σ(w'x+b) >= .5\n",
    "        return 1\n",
    "    else\n",
    "        return 0\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.525\n"
     ]
    }
   ],
   "source": [
    "mean_error = 0.0\n",
    "for i = 1:length(x_data)\n",
    "    mean_error += (predict(x_data[i], y_data[i], w, b) - y_data[i])^2\n",
    "end\n",
    "\n",
    "println(mean_error/length(x_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Closing statements:\n",
    "\n",
    "\n",
    "- We see that the Logistic Regression model can accurately predict the likelyhood of being admitted based on data and results using GPA, GMAT, and work experience. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.0",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
